{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Adversarial Example\n",
    "\n",
    "This notebook demonstrates how you can find adversarial examples for a pre-trained example network on the MNIST dataset.\n",
    "\n",
    "I have used `Gurobi` solver and it performance is significantly faster than other options. \n",
    "\n",
    "The `Images` package is only necessary for visualizing the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file C:\\Users\\BV SAMEER KUMAR\\.julia\\compiled\\v1.2\\MIPVerify\\VXcUH.ji for MIPVerify [e5e5f8be-2a6a-5994-adbb-5afbd0e30425]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package FileIO not found in current path:\n- Run `import Pkg; Pkg.add(\"FileIO\")` to install the FileIO package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package FileIO not found in current path:\n- Run `import Pkg; Pkg.add(\"FileIO\")` to install the FileIO package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at .\\loading.jl:876",
      " [2] top-level scope at In[1]:4"
     ]
    }
   ],
   "source": [
    "using MIPVerify\n",
    "using Gurobi\n",
    "using Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### MNIST dataset\n",
    "\n",
    "We start by loading the MNIST dataset. The data is provided as a Julia `struct` for easy access. The training images and test images are provided as a 4-dimensional array of size `(num_samples, height, width, num_channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mnist:\n",
       "  `train`: {LabelledImageDataset}\n",
       "    `images`: 60000 images of size (28, 28, 1), with pixels in [0.0, 1.0].\n",
       "    `labels`: 60000 corresponding labels, with 10 unique labels in [0, 9].\n",
       "  `test`: {LabelledImageDataset}\n",
       "    `images`: 10000 images of size (28, 28, 1), with pixels in [0.0, 1.0].\n",
       "    `labels`: 10000 corresponding labels, with 10 unique labels in [0, 9]."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = MIPVerify.read_datasets(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{LabelledImageDataset}\n",
       "    `images`: 60000 images of size (28, 28, 1), with pixels in [0.0, 1.0].\n",
       "    `labels`: 60000 corresponding labels, with 10 unique labels in [0, 9]."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(mnist.train.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Images.colorview` to preview these images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000-element Array{UInt8,1}:\n",
       " 0x05\n",
       " 0x00\n",
       " 0x04\n",
       " 0x01\n",
       " 0x09\n",
       " 0x02\n",
       " 0x01\n",
       " 0x03\n",
       " 0x01\n",
       " 0x04\n",
       " 0x03\n",
       " 0x05\n",
       " 0x03\n",
       "    ⋮\n",
       " 0x07\n",
       " 0x08\n",
       " 0x09\n",
       " 0x02\n",
       " 0x09\n",
       " 0x05\n",
       " 0x01\n",
       " 0x08\n",
       " 0x03\n",
       " 0x05\n",
       " 0x06\n",
       " 0x08"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Neural Network\n",
    "\n",
    "Import a sample pre-trained neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential net MNIST.n1\n",
       "  (1) Flatten(): flattens 4 dimensional input, with dimensions permuted according to the order [4, 3, 2, 1]\n",
       "  (2) Linear(784 -> 40)\n",
       "  (3) ReLU()\n",
       "  (4) Linear(40 -> 20)\n",
       "  (5) ReLU()\n",
       "  (6) Linear(20 -> 10)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = MIPVerify.get_example_network_params(\"MNIST.n1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MIPVerify.frac_correct` lets you to verify that the network has a reasonable accuracy on the test set of 96.95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mComputing fraction correct...100%|██████████████████████| Time: 0:00:03\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9695"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIPVerify.frac_correct(n1, mnist.test, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the first image into the neural net, obtaining the activations of the final softmax layer. \n",
    "\n",
    "Note that the image must be specified as a 4-dimensional array with size `(1, height, width, num_channels)`. There is a helper function `MIPVerify.get_image` that extracts the image from the dataset while preserving all four dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×28×28×1 Array{Float64,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 3, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 26, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 27, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 28, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = MIPVerify.get_image(mnist.test.images, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " -0.02074390040759505 \n",
       " -0.017499541361042703\n",
       "  0.16707187742051954 \n",
       " -0.05323712887827292 \n",
       " -0.019291011852467455\n",
       " -0.07951546424946399 \n",
       "  0.06191130931372918 \n",
       "  4.833970937815984   \n",
       "  0.46706000134294867 \n",
       "  0.40145201599055125 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_activations = sample_image |> n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category that has the largest activation is category 8, corresponding to a label of 7. Which means that the image we saw was of digit 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_activations |> MIPVerify.get_max_index) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIPVerify.get_label(mnist.test.labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an Adversarial Example\n",
    "\n",
    "We now try to find an adversarial example for the first image on `n1`, setting the target category as index `10` (corresponding to a true label of 9). Here we are trying to find an adversarial example that is does the following: if give adversarial as input to the nerual network we expect it ouput digit label as 7 but it instead it will tell us that the image label is 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[notice | MIPVerify]: Attempting to find adversarial example. Neural net predicted label is 8, target labels are [10]\u001b[39m\n",
      "\u001b[36m[notice | MIPVerify]: Rebuilding model from scratch. This may take some time as we determine upper and lower bounds for the input to each non-linear unit.\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m  Calculating upper bounds: 100%|███████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32m  Calculating lower bounds: 100%|███████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32m  Imposing relu constraint: 100%|███████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32m  Calculating upper bounds:  10%|███                    |  ETA: 0:00:20\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m  Calculating upper bounds: 100%|███████████████████████| Time: 0:01:01\u001b[39m\n",
      "\u001b[32m  Calculating lower bounds: 100%|███████████████████████| Time: 0:01:21\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[notice | MIPVerify]: The model built will be cached and re-used for future solves, unless you explicitly set rebuild=true.\u001b[39m\n",
      "Academic license - for non-commercial use only\n",
      "Optimize a model with 3385 rows, 3256 columns and 71132 nonzeros\n",
      "Variable types: 3196 continuous, 60 integer (60 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [2e-05, 7e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+02]\n",
      "  RHS range        [4e-03, 7e+02]\n",
      "\n",
      "MIP start did not produce a new incumbent solution\n",
      "MIP start violates constraint R1024 by 1.000000000\n",
      "\n",
      "Presolve removed 2956 rows and 2227 columns\n",
      "Presolve time: 0.27s\n",
      "Presolved: 429 rows, 1029 columns, 61365 nonzeros\n",
      "Variable types: 969 continuous, 60 integer (60 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 213 iterations, 0.02 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0    4          -    0.00000      -     -    0s\n",
      "Another try with MIP start\n",
      "H    0     0                      36.9304736    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   36.93047    0.00000   100%     -    0s\n",
      "H    0     0                      32.4680084    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   32.46801    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    5   32.46801    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    5   32.46801    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    5   32.46801    0.00000   100%     -    0s\n",
      "     0     2    0.00000    0    5   32.46801    0.00000   100%     -    0s\n",
      "H   61    59                      26.5540848    0.00000   100%  50.1    1s\n",
      "H   68    60                      23.6026873    0.00000   100%  49.5    1s\n",
      "H  156    71                       7.2019826    0.00000   100%  63.2    1s\n",
      "H  223    91                       6.6020445    0.00000   100%  54.4    1s\n",
      "*  491   200              37       6.3556350    0.00000   100%  39.9    2s\n",
      "*  998   395              36       6.0319390    0.00000   100%  45.9    3s\n",
      "  1741   604    3.76340   11    8    6.03194    0.00000   100%  51.2    5s\n",
      "H 2077   691                       5.7487698    0.00000   100%  53.0    6s\n",
      "H 2124   683                       5.7412895    0.00000   100%  52.8    7s\n",
      "  2701   753    0.79458   26   11    5.74129    0.00000   100%  55.4   10s\n",
      "* 3781   588              54       4.9728818    0.02050   100%  55.5   12s\n",
      "  4754   750    1.00199   38   12    4.97288    0.33754  93.2%  53.8   15s\n",
      "* 5135   868              52       4.6601251    0.42240  90.9%  52.2   15s\n",
      "  7396  1424    4.22625   31   12    4.66013    0.66181  85.8%  50.2   20s\n",
      "* 8443  1605              49       4.6418593    0.66565  85.7%  50.1   21s\n",
      " 10241  1898     cutoff   36         4.64186    0.89107  80.8%  49.8   25s\n",
      " 13069  2363    3.86003   22   12    4.64186    1.14308  75.4%  48.7   30s\n",
      " 14995  2493     cutoff   35         4.64186    1.59946  65.5%  46.6   35s\n",
      " 17901  2455    4.08311   27    4    4.64186    2.11692  54.4%  45.0   40s\n",
      " 20630  2410    3.04671   33    6    4.64186    2.38129  48.7%  44.9   45s\n",
      " 22995  2306    3.57771   33    3    4.64186    2.64637  43.0%  44.1   54s\n",
      " 23375  2363    3.57771   34    5    4.64186    2.64637  43.0%  43.9   55s\n",
      " 25923  2119    3.26155   20   12    4.64186    3.08461  33.5%  43.0   60s\n",
      " 28430  1390    4.36171   23   13    4.64186    3.63433  21.7%  42.1   65s\n",
      " 30667   373    4.21020   23    7    4.64186    4.21020  9.30%  41.7   70s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 4\n",
      "  Cover: 1\n",
      "  Projected implied bound: 31\n",
      "  MIR: 18\n",
      "  Flow cover: 20\n",
      "  Inf proof: 1\n",
      "\n",
      "Explored 31328 nodes (1299678 simplex iterations) in 71.37 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 10: 4.64186 4.66013 4.97288 ... 23.6027\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.641859316319e+00, best bound 4.641859316319e+00, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 11 entries:\n",
       "  :TargetIndexes      => [10]\n",
       "  :SolveTime          => 71.3779\n",
       "  :TotalTime          => 227.83\n",
       "  :PerturbedInput     => [:, :, 1, 1] =…\n",
       "  :Perturbation       => [:, :, 1, 1] =…\n",
       "  :TighteningApproach => \"mip\"\n",
       "  :PerturbationFamily => unrestricted\n",
       "  :SolveStatus        => :Optimal\n",
       "  :Model              => Minimization problem with:…\n",
       "  :Output             => JuMP.GenericAffExpr{Float64,JuMP.Variable}[-0.01206386…\n",
       "  :PredictedIndex     => 8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label_index = 10\n",
    "d = MIPVerify.find_adversarial_example(n1, sample_image, target_label_index, GurobiSolver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×28×28×1 Array{Float64,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 3, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 26, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 27, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 28, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP\n",
    "\n",
    "perturbed_sample_image = getvalue(d[:PerturbedInput])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we feed the perturbed image into the neural net and inspect the activation in the final layer. We verify that the perturbed image does maximize the activation of the target label index, which is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       "  1.0519768491269543 \n",
       "  0.78534384834018   \n",
       "  0.3138067335298753 \n",
       "  0.4270109980815697 \n",
       "  0.3697289881957214 \n",
       "  0.25614527786295416\n",
       "  0.6662295473279827 \n",
       "  4.567066220573279  \n",
       " -0.14798446025766632\n",
       "  4.567066220573276  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_sample_image |> n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the perturbed image and compare it to the original image. Since we are minimizing the L1-norm, changes are made to only a few pixels, but the magnitude of these changes are large (and noticeable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAHWSURBVGje7dm7axRRHMXxjyYpBBsNChY+KgstJIggJIKPRmNhYf6FpFHLgH+CWFpY2NsIQiAiIihEfBVpJL4xFioiCFqokEKIxYywkey644TdOz/vaebuzDBfzpw9l3sZsrKy+q11VW4+jws1get77TA+MKv56tjDBxhdY2D8WuQeZmX9h2o7l05gEh+xhKv4hDc1gfGntp4D22b4Frv+OPcNz/7ywA+4iPlUHMYHDra7MIl9eI49GMFhHMR7bG+59yc+Y1v5+52cYQ+1oofLOi9UNymynMeBlvNLeI0X2IyzuJyKw/jASnv8TjqNa3iKI/iSisP4wDXJcCsWyuMErqfkMD5wsP4jOIMt+IpXqTmMD6zdw1HcxZBizXMvNYfxgbV7OK7I7w4erXJ9Clf66TA+sFYPN+A+9uIoHqboMD6wVg+nFXuNW7rLb7kfDuMD/7mHJzGDHzhh9Xn0t1r3nfFfaTN6OIxLGMBNnfNj5R8l/itNv4cDeIz9WMTx8pisw/jAyhnuxstyfAqzqTuMD6w0l+7E7XI8jRtNcBgfWCnDKewox3OKtUryDuMDu87wEM410WF8YNcZjmFjOV7E96Y4jA+svLd4gmPaf1tKzmF8YFbz9QuU8T8T+c3qhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 reinterpret(Gray{Float64}, ::Array{Float64,2}):\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(1.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = colorview(Gray, perturbed_sample_image[1, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = imresize(img; ratio = 10.0)\n",
    "save(\"3.png\", new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAGrSURBVGje7dk/a9VQHIfxT711EFysKDhYOzl0KSKCoILiYtuhg30L10U7dnZ3dPAddBEEQRERKuigDl1E7T+8HVREEOqghaKFOiRDKVy9aUp78uN8l5z8IQ8PX06SQ8jJycnJycnJycnJyamfvm4nJtHGV6xjBt/wsSbwwF4bxgd27XAFQ9uO/cSH/9zwC+5gLhXD+MD+bifaGME8hnEGl3Een3Fyy7Ub+I4T5f4nucM9TF+Vi48oupzDuS3H17GMBQzgFu6lYhgfWKnDf+U67uM9rmA1FcP4wF3p8DjeldtJPEjJMD6wv/4tuIlj+IGl1AzjA2vPwwt4joOKb56XqRnGB9aeh2OK/mbxOkXD+MBaHR7CNfzGbfxJ0TA+sFaH04q1xlO8StUwPnDH78NxPMQaRvX2HN0Xw/jAHc3Do7iLFp7ovb99MYwPrDwPW3iDs+go3oedlA3jAyt3eBqL5XgCj1I3jA+s9Cw9hWfleBqPm2AYH1ipwxsYLMcvsNkEw/jAnju8hKkmGsYH9tzhRRwuxx38aophfGDltcVbXNX931JyhvGBOc3PX/q9Oc17OzXKAAAAAElFTkSuQmCC",
      "text/plain": [
       "28×28 reinterpret(Gray{Float64}, ::Array{Float64,2}):\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = colorview(Gray, sample_image[1, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = imresize(img; ratio = 10.0)\n",
    "save(\"4.png\", new_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
